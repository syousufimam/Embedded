{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "goose-pytorch1_8-detectron2-object-detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syousufimam/Embedded/blob/master/goose_pytorch1_8_detectron2_object_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT4WcI4BNy7w"
      },
      "source": [
        "Sign up for Sense Data Annotation: [http://sixgill.tech/ai-powered-labeling](http://sixgill.tech/ai-powered-labeling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2BjcHEzIwi3"
      },
      "source": [
        "# Welcome to Google Colab\n",
        "\n",
        "Colab is essentially Google's way of hosting a [jupyter notebook](https://jupyter.org/). A very popular tool to use as a data scientist!\n",
        "\n",
        "It allows us to write code, documentation, and output visuals all in one place.\n",
        "\n",
        "To be able to and edit the code in this workshop. Please make a copy for yourself\n",
        "\n",
        "`file > save a copy in drive`\n",
        "\n",
        "This should open a new tab with your own copy of this notebook. It can take a minute to load.\n",
        "\n",
        "Colab comes with a lot of great data science libraries pre-installed. We'll show to start working with them in part-2 \n",
        "\n",
        "Colab also gives you some options for running complicated computations such as training deep learning model. To see access those options:\n",
        "\n",
        "`Runtime > change runtime type` Select `GPU`, `TPU`, or `None`\n",
        "\n",
        "We don't need to change anything for this workshop, but its a great resource if you start learning deep learning and don't have a powerful GPU at home. \n",
        "\n",
        "This is a text cell. \n",
        "\n",
        "You can add a new text cell by clicking `+ Text` above. \n",
        "\n",
        "It does not highlight wrong spelling. I apologize for any typos!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuBDcHDNIwr8"
      },
      "source": [
        "### Code Cells\n",
        "\n",
        "Below is a code cell. There is nothing in it right now.\n",
        "\n",
        "To run a code cell click on it and then click the play button. Or press `shift+enter`\n",
        "\n",
        "You can add new code cells by clicking the ` + Code ` button above\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4K9fzYVCS_J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5662485-65a7-41d4-dd3b-d264f1721fce"
      },
      "source": [
        "print(\"Hello world!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello world!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2dTv7a-fzGz"
      },
      "source": [
        "### Run Terminal Commands\n",
        "\n",
        "Colab actually gives you access to a whole ubuntu instance!\n",
        "\n",
        "You can run terminal commands by putting `!` before the command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zOMVSt2fzQ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afcbe4a0-fe0e-4b66-88cd-4337fc0d131f"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gudGfIFtYtgM"
      },
      "source": [
        "# Install Detectron2 and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AdA1xihtK9L"
      },
      "source": [
        "# Install torch 1.8 for detectron2 ( if colab  changes versions)\n",
        "\n",
        "!pip install torch==1.8.0+cu101 torchvision==0.9.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r25jgD15Yt6N"
      },
      "source": [
        "# install dependencies: \n",
        "!pip install pyyaml==5.1 pycocotools>=2.0.1\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "!gcc --version\n",
        "# opencv is pre-installed on colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwDj2V3mtERL"
      },
      "source": [
        "torch.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xpSG4obaFxm"
      },
      "source": [
        "torchvision.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW6klfd0G0JA"
      },
      "source": [
        "# # install detectron2: (Colab has CUDA 10.1 + torch 1.7)\n",
        "# # See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "assert torch.__version__.startswith(\"1.8\")\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnGSak1SEuwx"
      },
      "source": [
        "# Make sure compatible pillow version\n",
        "!pip install pillow==7.2\n",
        "# %reload_ext autoreload\n",
        "# %autoreload"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPIEyyjiY4jh"
      },
      "source": [
        "# Some basic setup:\n",
        "# Import detectron2 & common detectron2 utilities\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import tqdm\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha8dLsIGZK5g"
      },
      "source": [
        "# Creating a Dataset\n",
        "\n",
        "[Sign Up](http://sixgill.tech/ai-powered-labeling) for Sense Data Annotation. No card required.\n",
        "\n",
        "Labeled with [Sense Data Annotation](https://sixgill.com/platform/sense-data-annotation/) and exported in the COCO format.\n",
        "\n",
        "Read Sense Data Annotation docs [here](https://docs.sixgill.com/).\n",
        "\n",
        "Give me any feedback / ask questions on [slack](https://sixgill.tech/slack)\n",
        "\n",
        "![Labeling for object detection](https://gblobscdn.gitbook.com/assets%2F-MJxxV6sKASlEjxiEwc2%2F-MKrhpy2wLI2lxD79AhV%2F-MKrinYxj-fQSlHaP6jB%2FSmartPoly_2cows.gif?alt=media&token=667d94a8-2493-4e5c-846c-0bae50480ba4)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCGXEcWTpe1w"
      },
      "source": [
        "# Download our already labeled Dataset\n",
        "\n",
        "For this project we're going to use my \"famous\" synthetic goose dataset. Read more about how and why that was created here: https://github.com/sagecodes/make-a-wildlife-object-detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY1CU9GKZOnf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "621d174f-4da5-4ae2-9caf-2ed451e358e7"
      },
      "source": [
        "# download dataset from github\n",
        "!git clone https://github.com/sagecodes/geese-object-detection-dataset.git\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'geese-object-detection-dataset'...\n",
            "remote: Enumerating objects: 143, done.\u001b[K\n",
            "remote: Total 143 (delta 0), reused 0 (delta 0), pack-reused 143\u001b[K\n",
            "Receiving objects: 100% (143/143), 47.74 MiB | 25.10 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv_1o3JBZtMD"
      },
      "source": [
        "!ls geese-object-detection-dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IThQWAiw43Gd"
      },
      "source": [
        "!ls geese-object-detection-dataset/img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2Dd9LNIpv0-"
      },
      "source": [
        "## Import a Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHbb-G1EZkQ-"
      },
      "source": [
        "### Setup COCO dataset for detectron2\n",
        "\n",
        "[Detectron2 dataset docs](https://detectron2.readthedocs.io/tutorials/datasets.html)\n",
        "\n",
        "[COCO](https://cocodataset.org/#home) is a popular dataset and computer vision dataset format\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC7d7CeDZeyx"
      },
      "source": [
        "For this workshop we're using git clone, but for your own data you can use a couple other ways\n",
        "\n",
        "- directly upload to colab instance\n",
        "- mount to google drive <-- strongly encouraged for long term projects \n",
        "\n",
        "<-- You can easily to do both from the side bar\n",
        "\n",
        "or you can mount with a code cell\n",
        "```\n",
        "drive.mount('/content/gdrive')\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UsMCe_SZfWI"
      },
      "source": [
        "# import COCO format datasets\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"my_dataset_train\", {}, \"/content/geese-object-detection-dataset/train.json\", \"/content/geese-object-detection-dataset\")\n",
        "register_coco_instances(\"my_dataset_val\", {}, \"/content/geese-object-detection-dataset/validation.json\", \"/content/geese-object-detection-dataset\")\n",
        "\n",
        "# bonus labled dataset for getting more metrics on our model\n",
        "register_coco_instances(\"my_dataset_test\", {}, \"/content/geese-object-detection-dataset/test.json\", \"/content/geese-object-detection-dataset\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD7Pb3Y6aDa-"
      },
      "source": [
        "# check dataset is reading correctly\n",
        "my_dataset_train_metadata = MetadataCatalog.get(\"my_dataset_train\")\n",
        "\n",
        "dataset_dicts = DatasetCatalog.get(\"my_dataset_train\")\n",
        "\n",
        "for d in random.sample(dataset_dicts, 3):\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=my_dataset_train_metadata, scale=0.5)\n",
        "    out = visualizer.draw_dataset_dict(d)\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])\n",
        "\n",
        "# set meta data for validation set (we'll usue this later for testing)\n",
        "test_metadata = MetadataCatalog.get(\"my_dataset_test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN1BasPxtpwu"
      },
      "source": [
        "You can learn more about this dataset [here](https://github.com/sagecodes/make-a-wildlife-object-detector)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0pyznpv9Zdg"
      },
      "source": [
        "# Let look at what our dataset looks like after loading in\n",
        "dataset_dicts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeGk_1sXmaR8"
      },
      "source": [
        "# Training a Detectron2 Model for Object Detection "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoLeGjt74UMP"
      },
      "source": [
        "Setup Validation class\n",
        "\n",
        "https://medium.com/@apofeniaco/training-on-detectron2-with-a-validation-set-and-plot-loss-on-it-to-avoid-overfitting-6449418fbf4e"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf5ofcUxvP25"
      },
      "source": [
        "# Create class for doing validation during training\n",
        "# Read more about this above: \n",
        "\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.evaluation import COCOEvaluator\n",
        "\n",
        "class MyTrainer(DefaultTrainer):\n",
        "    @classmethod\n",
        "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "        if output_folder is None:\n",
        "            output_folder = os.path.join(cfg.OUTPUT_DIR,\"inference\")\n",
        "        return COCOEvaluator(dataset_name, cfg, True, output_folder)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TxBYfQLajFu"
      },
      "source": [
        "https://detectron2.readthedocs.io/tutorials/configs.html\n",
        "\n",
        "model zoo: https://github.com/facebookresearch/Detectron/blob/master/MODEL_ZOO.md\n",
        "\n",
        "\n",
        "**Some model types to try**\n",
        "\n",
        "- Instance Segmentation: `COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml`\n",
        "- Object Detection: `COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml` \n",
        "\n",
        "\n",
        "The warning: `Skip loading parameter 'roi_heads.box_predictor.cls_score.weight to the model due to incompatible shapes` is OK. It's because we have different number of classes than what the weights were trained on (81 clasess)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doRkGMG0anEe"
      },
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "\n",
        "# initialize Detectron2 Training configuration\n",
        "cfg = get_cfg()\n",
        "# Select Model from Model Zoo \n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
        "# Let training initialize from pretrained model zoo weights (transfer learning!)\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\")\n",
        "# Select how many classes you're training to detect\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
        "# Pass in training dataset (don't forget comma at end)\n",
        "cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n",
        "# Number of iteration to learn from our dataset\n",
        "cfg.SOLVER.MAX_ITER = 300\n",
        "# Learning Rate (think: how your network adjusts weights)\n",
        "cfg.SOLVER.BASE_LR = 0.001\n",
        "# Can adjust for Multi processes loading\n",
        "cfg.DATALOADER.NUM_WORKERS = 0\n",
        "# Images per batch per GPU\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "# Number of instances per btach\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 32\n",
        "\n",
        "# Create output directory for training\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Load validation dataset\n",
        "cfg.DATASETS.TEST = (\"my_dataset_val\",)\n",
        "# number of iteration before validation\n",
        "cfg.TEST.EVAL_PERIOD = 50\n",
        "\n",
        "trainer = MyTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bUKdgVjaps7"
      },
      "source": [
        "# Look at training curves in tensorboard:\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC5mATiFa102"
      },
      "source": [
        "# Use a Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnZbMz-Ra_KH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f3d64bf-25e7-4f23-b36c-b9f7ad44a3b2"
      },
      "source": [
        "!ls ./output/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "events.out.tfevents.1616555056.fa6a01c9bc9f.58.0  metrics.json\n",
            "inference\t\t\t\t\t  model_final.pth\n",
            "last_checkpoint\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6JbVr2FVXcm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiGqv7K2bC88"
      },
      "source": [
        "# Initialize config\n",
        "cfg = get_cfg()\n",
        "# Must be same as model trained in.\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
        "# Path to saved model weights (trained model)\n",
        "cfg.MODEL.WEIGHTS = '/content/output/model_final.pth'\n",
        "# if using datasets for metadata\n",
        "cfg.DATASETS.TEST = (\"my_dataset_test\", )\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
        "# Threshhold to show predictions\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9\n",
        "# create predictor instance to use\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G5kvcr1bMt6"
      },
      "source": [
        "# Run test on an image\n",
        "# from detectron2.utils.visualizer import ColorMode\n",
        "test_image_path = '/content/geese-object-detection-dataset/img/01ES4X7WSQF4XAGHFFACJSPN4Q.jpeg'\n",
        "im = cv2.imread(test_image_path)\n",
        "outputs = predictor(im)\n",
        "v = Visualizer(im[:, :, ::-1],\n",
        "                metadata=test_metadata, \n",
        "                scale=0.5\n",
        "    )\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRtd0pmo9pXZ"
      },
      "source": [
        "# What does an image look like to the computer?\n",
        "print(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_c4QsDWbRxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e800ab96-2e10-4e0f-a95d-75a33bf7213f"
      },
      "source": [
        "#What does the model output look like not drawn on an image?\n",
        "print(outputs[\"instances\"].to(\"cpu\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Instances(num_instances=2, image_height=600, image_width=800, fields=[pred_boxes: Boxes(tensor([[277.9910, 140.4599, 506.6395, 275.7843],\n",
            "        [150.4610, 192.7703, 377.7130, 408.9513]])), scores: tensor([0.9947, 0.9812]), pred_classes: tensor([0, 0])])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oH1PuVVbXBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01c3cbd5-776c-4a3f-caed-3712906ad21e"
      },
      "source": [
        "# Count objects detected in an image\n",
        "object_count = outputs[\"instances\"].to(\"cpu\")\n",
        "print(\"Detected \"+str(len(object_count))+\" Geese\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detected 2 Geese\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8w36kSteEL4"
      },
      "source": [
        "# Run test on an image\n",
        "# from detectron2.utils.visualizer import ColorMode\n",
        "test_image_path = '/content/geese-object-detection-dataset/img/01ES4X8G607FY4FP9FV0E3WNNA.png'\n",
        "im = cv2.imread(test_image_path)\n",
        "outputs = predictor(im)\n",
        "v = Visualizer(im[:, :, ::-1],\n",
        "                metadata=test_metadata, \n",
        "                scale=0.5\n",
        "    )\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJt9ULuV1EYW"
      },
      "source": [
        "### Evaluation metrics\n",
        "\n",
        "Here is great post that will go deeper into object detection evaluations metrics:\n",
        "\n",
        "https://cocodataset.org/#detection-eval \n",
        "\n",
        "https://www.pyimagesearch.com/2018/05/14/a-gentle-guide-to-deep-learning-object-detection/\n",
        "\n",
        "Detectron2 has built evaluation methods "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isw1MqJAbZPd"
      },
      "source": [
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "evaluator = COCOEvaluator(\"my_dataset_test\", cfg, False, output_dir=\"./output/\")\n",
        "val_loader = build_detection_test_loader(cfg, \"my_dataset_test\")\n",
        "print(inference_on_dataset(trainer.model, val_loader, evaluator))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yMoPnEs1Uyj"
      },
      "source": [
        "# Run detection on a video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAMkQGbCbbcp"
      },
      "source": [
        "# This is the video we're going to process\n",
        "from IPython.display import YouTubeVideo, display\n",
        "video = YouTubeVideo(\"6vmlPXkkQ4Y\", width=500)\n",
        "display(video)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZajwly2bfOr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4770e31-a03e-4d36-901c-8f36f6da6445"
      },
      "source": [
        "# Install dependencies, to download video, and crop for processing \n",
        "# Can skip if you are uploading your own video from google drive\n",
        "!pip install youtube-dl\n",
        "# !pip uninstall -y opencv-python-headless opencv-contrib-python\n",
        "# !apt install python3-opencv  # the one pre-installed have some issues"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting youtube-dl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/01/0cddb3c14dec91d627a0c9df61e072812e3e3b8cee48a95e6566fd403a79/youtube_dl-2021.3.14-py2.py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 4.2MB/s \n",
            "\u001b[?25hInstalling collected packages: youtube-dl\n",
            "Successfully installed youtube-dl-2021.3.14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGQChDQ_bhKm"
      },
      "source": [
        "# Download video from youtube\n",
        "!youtube-dl https://www.youtube.com/watch?v=6vmlPXkkQ4Y -f 22 -o video.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crNkWBFwf1bx"
      },
      "source": [
        "# (optional) create clip from video. Useful for large videos\n",
        "!ffmpeg -i /content/video.mp4 -t 00:00:06 -c:v copy video-clip.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RXv-vCbbkEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd812d1c-df60-4d8a-efed-a5ee782aa91d"
      },
      "source": [
        "from detectron2.utils.video_visualizer import VideoVisualizer\n",
        "from detectron2.utils.visualizer import ColorMode#, Visualizer\n",
        "\n",
        "# Extract video properties\n",
        "video = cv2.VideoCapture('/content/video.mp4')\n",
        "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "frames_per_second = video.get(cv2.CAP_PROP_FPS)\n",
        "num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "# Initialize video writer\n",
        "video_writer = cv2.VideoWriter('out.mp4',\n",
        "                               fourcc=cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
        "                               fps=float(frames_per_second),\n",
        "                               frameSize=(width, height),\n",
        "                               isColor=True)\n",
        "\n",
        "# Initialize visualizer\n",
        "v = VideoVisualizer(MetadataCatalog.get(cfg.DATASETS.TEST[0]),\n",
        "                                        ColorMode.IMAGE)\n",
        "\n",
        "def run_model_on_video(video, maxFrames):\n",
        "    \"\"\" \n",
        "    Runs the predictor on every frame in the video (unless maxFrames is given),\n",
        "    and returns the frame with the predictions drawn.\n",
        "    \"\"\"\n",
        "\n",
        "    readFrames = 0\n",
        "    while True:\n",
        "        hasFrame, frame = video.read()\n",
        "        if not hasFrame:\n",
        "            break\n",
        "\n",
        "        # Get prediction results for this frame\n",
        "        outputs = predictor(frame)\n",
        "\n",
        "        # Make sure the frame is colored\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        # Draw a visualization of the predictions using the video visualizer\n",
        "        visualization = v.draw_instance_predictions(frame,\n",
        "                                                    outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "        # Convert Matplotlib RGB format to OpenCV BGR format\n",
        "        visualization = cv2.cvtColor(visualization.get_image(),\n",
        "                                     cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        yield visualization\n",
        "\n",
        "        readFrames += 1\n",
        "        if readFrames > maxFrames:\n",
        "            break\n",
        "\n",
        "# Enumerate the frames of the video\n",
        "for visualization in tqdm.tqdm(run_model_on_video(video, num_frames),\n",
        "                               total=num_frames):\n",
        "\n",
        "    # Write to video file\n",
        "    video_writer.write(visualization)\n",
        "\n",
        "# Release resources\n",
        "video.release()\n",
        "video_writer.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 307/307 [01:37<00:00,  3.14it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvPMNaOsgy15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7aa64c4d-48bc-4e98-e74e-5632615921f6"
      },
      "source": [
        "# Download the results\n",
        "from google.colab import files\n",
        "files.download('out.mp4')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_61111759-a449-49ee-8b7e-4387d584aae4\", \"out.mp4\", 26876227)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_35-wWaqs8Nk"
      },
      "source": [
        "# Keep Learning!\n",
        "\n",
        "We just scratched the surface of the computer vision field, but I hope you feel confidant and inspired to learn more about it and creat your own projects! \n",
        "\n",
        "[Sign up for Sense Data Annotation](http://sixgill.tech/ai-powered-labeling) & Start Labeling your own datasets! Retrain this notebook with them.\n",
        "\n",
        "[Join me in slack](https://bit.ly/30lO9Cq) to ask any questions.\n",
        "\n",
        "Follow Sixgill on [Medium](https://medium.com/sixgill) for Computer Vision Tutorials\n",
        "\n",
        "Subscribe on [youtube](https://www.youtube.com/channel/UCWt--5NcY-O9PhjfPSDotew) for more content around computer vision.\n",
        "\n",
        "Read/watch [my talk](https://github.com/sagecodes/make-a-wildlife-object-detector) on how to think about data and create sythetic data to improve your models.\n",
        "\n",
        "\n",
        "### Learn more about different object detection methods:\n",
        "\n",
        "\n",
        "- Single Shot MultiBox Detector (SSD) [explained](https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab) | [paper](https://arxiv.org/abs/1512.02325)\n",
        "\n",
        "- You Only Look Once (YOLO) [Explained](https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088) | [Paper](https://arxiv.org/abs/1506.02640) \n",
        "\n",
        "- Mask R-CNN [Explained](https://medium.com/@jonathan_hui/image-segmentation-with-mask-r-cnn-ebe6d793272) | [Paper](https://arxiv.org/abs/1703.06870)\n",
        "\n",
        "- Detectron2 [github](https://github.com/facebookresearch/detectron2) | [official coloab](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5)\n",
        "\n",
        "[Pyimagesearch](https://www.pyimagesearch.com/category/object-detection/) and [Machine Learning Mastery](https://machinelearningmastery.com/) are a great resources to take a look at. \n",
        "\n",
        "\n",
        "-------------------\n",
        "\n",
        "#### Raw Links for chat:\n",
        "http://sixgill.tech/ai-powered-labeling\n",
        "\n",
        "https://www.linkedin.com/in/sageelliott/\n",
        "\n",
        "https://twitter.com/sagecodes\n",
        "\n",
        "https://github.com/sagecodes\n",
        "\n",
        "https://medium.com/sixgill\n",
        "\n",
        "https://github.com/sagecodes/make-a-wildlife-object-detector\n",
        "\n",
        "https://sixgill.tech/slack\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JngC9-2nvoQa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}